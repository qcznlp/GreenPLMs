{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    ".text_cell_render {\n",
    "font-family: Times New Roman, serif;\n",
    "}\n",
    "</style>\n",
    "**Part 1: Translate English BERT-Large-Whole-Word-Masking Vocabulary into Chinese via ECDICT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Generate English-Chinese dictionary via ecdict.\n",
    "# ecdict is from https://github.com/skywind3000/ECDICT\n",
    "df = pd.read_csv(\"E:\\\\Steve_Zeng_Related\\\\YLab\\\\Translation_BERT_project\\\\code_project\\\\data\\\\ecdict.txt\",sep=\",\")\n",
    "zh_en_dict = dict(zip(df.word, df.translation))\n",
    "\n",
    "def read_vocabulary_and_translate(vocab_path,new_path,bi_dict):\n",
    "    '''\n",
    "    Accepts a BERT-wwm vocabulary and generates its translation\n",
    "    vocab_path: BERT-wwm vocabulary path\n",
    "    new_path: Translated vocabulary path\n",
    "    bi_dict: Bilingual dictionary, such as zh_en_dict above\n",
    "    '''\n",
    "    word_list = []\n",
    "    new_word_list = []\n",
    "    with open(vocab_path,encoding=\"UTF-8\") as vocab:\n",
    "        vocab_contents = vocab.readlines()\n",
    "        for line in vocab_contents:\n",
    "            word = line.strip()\n",
    "            word_list.append(word)\n",
    "            if word.isdigit():\n",
    "                new_word = word\n",
    "            elif word in zh_en_dict.keys():\n",
    "                new_word = bi_dict[word]\n",
    "            elif word.lower() in zh_en_dict.keys():\n",
    "                new_word = bi_dict[word.lower()]\n",
    "            else:\n",
    "                new_word = word\n",
    "            new_word_list.append(new_word)\n",
    "    with open(new_path,'a',encoding=\"UTF-8\") as obj:\n",
    "        for item in new_word_list:\n",
    "            obj.write(item + \"\\n\")\n",
    "\n",
    "# Generate trans.txt\n",
    "read_vocabulary_and_translate(\"E:\\\\Steve_Zeng_Related\\\\YLab\\\\Translation_BERT_project\\\\code_project\\\\BERT\\\\bert-large-cased-whole-word-masking\\\\vocab.txt\",\n",
    "                            \"E:\\\\Steve_Zeng_Related\\\\YLab\\\\Translation_BERT_project\\\\code_project\\\\data\\\\trans_dict.txt\",zh_en_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    ".text_cell_render {\n",
    "font-family: Times New Roman, serif;\n",
    "}\n",
    "</style>\n",
    "**Part 2: Process Chinese-English Dictionary into available Python data structure and clean the dictionary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Chinese-English dictionary\n",
    "# Dictionary is from https://www.mdbg.net/chinese/dictionary?page=cedict\n",
    "import jionlp as jio\n",
    "import texthero as hero\n",
    "import re\n",
    "\n",
    "with open('E:\\\\Steve_Zeng_Related\\\\YLab\\\\Translation_BERT_project\\\\code_project\\\\data\\\\cedict_ts.u8',encoding=\"UTF-8\") as file:\n",
    "    cedict = file.readlines()\n",
    "\n",
    "def parse_line(line):\n",
    "    parsed = {}\n",
    "    if line == '':\n",
    "        cedict.remove(line)\n",
    "        return 0\n",
    "    if line.startswith(\"#\"):\n",
    "        cedict.remove(line)\n",
    "        return 0\n",
    "    line = line.rstrip('/')\n",
    "    line = line.split('/')\n",
    "    if len(line) <= 1:\n",
    "        return 0\n",
    "    english = line[1]\n",
    "    if \"/\" in english:\n",
    "        english = english.split(\"/\")[0]\n",
    "    char_and_pinyin = line[0].split('[')\n",
    "    characters = char_and_pinyin[0]\n",
    "    characters = characters.split()\n",
    "    traditional = characters[0]\n",
    "    simplified = characters[1]\n",
    "    pinyin = char_and_pinyin[1]\n",
    "    pinyin = pinyin.rstrip()\n",
    "    pinyin = pinyin.rstrip(\"]\")\n",
    "    parsed['traditional'] = traditional\n",
    "    parsed['simplified'] = simplified\n",
    "    parsed['pinyin'] = pinyin\n",
    "    parsed['english'] = english\n",
    "    return parsed\n",
    "\n",
    "def is_contain_chinese(check_str):\n",
    "    \"\"\"\n",
    "    判断字符串中是否包含中文\n",
    "    :param check_str: {str} 需要检测的字符串\n",
    "    :return: {bool} 包含返回True， 不包含返回False\n",
    "    \"\"\"\n",
    "    for ch in check_str:\n",
    "        if u'\\u4e00' <= ch <= u'\\u9fff':\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def clean_parsed_dict(parsed_dict):\n",
    "    simple_punctuation = '[;|.]'\n",
    "    for k,v in list(parsed_dict.items()):\n",
    "        clean = jio.remove_parentheses(v)\n",
    "        no_punctuation = re.sub(simple_punctuation, '', clean)\n",
    "        parsed_dict[k] = no_punctuation\n",
    "    for key,value in list(parsed_dict.items()):\n",
    "        if is_contain_chinese(value):\n",
    "            del parsed_dict[key]\n",
    "        elif \"lit\" in value:\n",
    "            del parsed_dict[key]\n",
    "        elif \",\" in value:\n",
    "            parsed_dict[key] = value.split(\",\")[0].strip()\n",
    "    return parsed_dict\n",
    "\n",
    "# Generate cedict.txt\n",
    "parsed_dict = {}\n",
    "for line in cedict:\n",
    "    part_parsed = parse_line(line)\n",
    "    simplified = part_parsed['simplified']\n",
    "    english = part_parsed[\"english\"]\n",
    "    parsed_dict[simplified] = english\n",
    "parsed_dict = clean_parsed_dict(parsed_dict)\n",
    "file = open('E:\\\\Steve_Zeng_Related\\\\YLab\\\\Translation_BERT_project\\\\code_project\\\\data\\\\cedict_jio.txt', 'w',encoding=\"UTF-8\") \n",
    "for k,v in parsed_dict.items():\n",
    "    file.write(str(k)+' '+str(v)+'\\n')\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    ".text_cell_render {\n",
    "font-family: Times New Roman, serif;\n",
    "}\n",
    "</style>\n",
    "**Part 3: Translate English BERT-base-uncased vocabulary into Chinese via Baidu Translate and clean it**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# This code shows an example of text translation from English to Simplified-Chinese.\n",
    "# This code runs on Python 2.7.x and Python 3.x.\n",
    "# You may install `requests` to run this code: pip install requests\n",
    "# Please refer to `https://api.fanyi.baidu.com/doc/21` for complete api document\n",
    "\n",
    "import requests\n",
    "import random\n",
    "import json\n",
    "from hashlib import md5\n",
    "\n",
    "# Set your own appid/appkey.\n",
    "appid = '20210926000957196'\n",
    "appkey = 'tPgtGyQQuvfgsAMbbUvK'\n",
    "\n",
    "# For list of language codes, please refer to `https://api.fanyi.baidu.com/doc/21`\n",
    "from_lang = 'en'\n",
    "to_lang =  'zh'\n",
    "\n",
    "endpoint = 'http://api.fanyi.baidu.com'\n",
    "path = '/api/trans/vip/translate'\n",
    "url = endpoint + path\n",
    "\n",
    "# query = 'Hello World! This is 1st paragraph.\\nThis is 2nd paragraph.'\n",
    "\n",
    "# Generate salt and sign\n",
    "def make_md5(s, encoding='utf-8'):\n",
    "    return md5(s.encode(encoding)).hexdigest()\n",
    "\n",
    "new_word_list = []\n",
    "with open(\"E:\\\\Steve_Zeng_Related\\\\YLab\\\\Translation_BERT_project\\\\code_project\\\\BERT\\\\bert-base-uncased\\\\vocab.txt\",\"r\",encoding=\"UTF-8\") as file:\n",
    "    contents = file.readlines()\n",
    "for item in contents:\n",
    "    item = item.strip()\n",
    "    if item.startswith(\"##\"):\n",
    "        new_word_list.append(item)\n",
    "        continue\n",
    "    elif item.startswith(\"[\"):\n",
    "        new_word_list.append(item)\n",
    "        continue\n",
    "    else:\n",
    "        salt = random.randint(32768, 65536)\n",
    "        sign = make_md5(appid + item + str(salt) + appkey)\n",
    "        headers = {'Content-Type': 'application/x-www-form-urlencoded'}\n",
    "        payload = {'appid': appid, 'q': item, 'from': from_lang, 'to': to_lang, 'salt': salt, 'sign': sign}\n",
    "        r = requests.post(url, params=payload, headers=headers)\n",
    "        result = r.json()\n",
    "        json_string = json.dumps(result, indent=4, ensure_ascii=False)\n",
    "        string = json.loads(json_string)\n",
    "        try:\n",
    "            trans_word = string[\"trans_result\"][0][\"dst\"]\n",
    "            new_word_list.append(trans_word)\n",
    "        except KeyError:\n",
    "            new_word_list.append(item)\n",
    "    \n",
    "    with open(\"E:\\\\Steve_Zeng_Related\\\\YLab\\\\Translation_BERT_project\\\\code_project\\\\data\\\\BERT-base-uncased-trans.txt\",\"w\",encoding=\"UTF-8\") as file:\n",
    "        for new_word in new_word_list:\n",
    "            file.write(new_word + \"\\n\")\n",
    "\n",
    "import texthero as hero\n",
    "import jionlp as jio\n",
    "import re\n",
    "\n",
    "processed = []\n",
    "with open(\"E:\\\\Steve_Zeng_Related\\\\YLab\\\\Translation_BERT_project\\\\code_project\\\\data\\\\BERT-base-uncased-trans.txt\",\"r\",encoding=\"UTF-8\") as f:\n",
    "    contents = f.readlines()\n",
    "for i in contents:\n",
    "    i = i.strip()\n",
    "    if len(i) == 1:\n",
    "        processed.append(i)\n",
    "    elif i.startswith(\"[\"):\n",
    "        processed.append(i)\n",
    "    elif i.startswith(\"#\"):\n",
    "        processed.append(i)\n",
    "    else:\n",
    "        no_parenthesis = jio.remove_parentheses(i)\n",
    "        no_punctuation = re.sub(r'[^\\w\\s]', '', no_parenthesis)\n",
    "        no_space = no_punctuation.replace(\" \",\"\")\n",
    "        processed.append(no_space)\n",
    "\n",
    "with open(\"E:\\\\Steve_Zeng_Related\\\\YLab\\\\Translation_BERT_project\\\\code_project\\\\data\\\\BERT-base-uncased-trans-processed.txt\",\"w\",encoding=\"UTF-8\") as file:\n",
    "        for process in processed:\n",
    "            file.write(process + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    ".text_cell_render {\n",
    "font-family: Times New Roman, serif;\n",
    "}\n",
    "</style>\n",
    "**Part 4: Clean English-Chinese dictionary generated from Part 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"E:\\\\Steve_Zeng_Related\\\\YLab\\\\Translation_BERT_project\\\\code_project\\\\data\\\\trans_dict.txt\",\"r\",encoding=\"UTF-8\") as f:\n",
    "    trans_dict_contents = f.readlines()\n",
    "\n",
    "def containenglish(test_string):\n",
    "    return bool(re.search('[a-zA-Z]', test_string))\n",
    "\n",
    "without_line = []\n",
    "for item in trans_dict_contents:\n",
    "    item = item.strip()\n",
    "    if \"\\\\\" in item:\n",
    "        new_item = item.split(\"\\\\\")[0].strip()\n",
    "        without_line.append(new_item)\n",
    "    else:\n",
    "        without_line.append(item)\n",
    "\n",
    "no_role = []\n",
    "for no_l in without_line:\n",
    "    if len(no_l) != 1 and \".\" in no_l:\n",
    "        without_role = no_l.split(\".\")[1]\n",
    "        no_role.append(without_role)\n",
    "    else:\n",
    "        no_role.append(no_l)\n",
    "    \n",
    "no_brackets = []\n",
    "for no_r in no_role:\n",
    "    if is_contain_chinese(no_r):\n",
    "        without_b = jio.remove_parentheses(no_r)\n",
    "        no_brackets.append(without_b)\n",
    "    else:\n",
    "        no_brackets.append(no_r)\n",
    "\n",
    "no_punctuation = []\n",
    "simple_punctuation = '[;,；，]'\n",
    "for no_b in no_brackets:\n",
    "    without_punctuation = re.sub(simple_punctuation, ' ', no_b)\n",
    "    no_punctuation.append(without_punctuation)\n",
    "\n",
    "no_space = []\n",
    "for no_pu in no_punctuation:\n",
    "    no_pu = no_pu.strip()\n",
    "    if \" \" in no_pu:\n",
    "        without_s = no_pu.split(\" \")[0]\n",
    "        no_space.append(without_s)\n",
    "    else:\n",
    "        no_space.append(no_pu)\n",
    "\n",
    "no_tense_things = []\n",
    "for no_s in no_space:\n",
    "    if containenglish(no_s) and \"的\" in no_s:\n",
    "        without_tense = no_s.split(\"的\")[0]\n",
    "        no_tense_things.append(without_tense)\n",
    "    else:\n",
    "        no_tense_things.append(no_s)\n",
    "\n",
    "with open(\"E:\\\\Steve_Zeng_Related\\\\YLab\\\\Translation_BERT_project\\\\code_project\\\\data\\\\BERT-large-wwm-dict-processed.txt\",\"w\",encoding=\"UTF-8\") as file:\n",
    "        for ntt in no_tense_things:\n",
    "            file.write(ntt + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    ".text_cell_render {\n",
    "font-family: Times New Roman, serif;\n",
    "}\n",
    "</style>\n",
    "**Part 5: Check the difference between CEDICT-jio and WoBERT vocabulary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9293\n"
     ]
    }
   ],
   "source": [
    "with open('E:\\\\Steve_Zeng_Related\\\\YLab\\\\Translation_BERT_project\\\\code_project\\\\data\\\\cedict_jio.txt', 'r',encoding=\"UTF-8\") as cedictjio:\n",
    "    contents = cedictjio.readlines()\n",
    "\n",
    "with open('E:\\\\Steve_Zeng_Related\\\\YLab\\\\Translation_BERT_project\\\\code_project\\\\BERT\\\\chinese_wobert_L-12_H-768_A-12\\\\vocab.txt', 'r',encoding=\"UTF-8\") as wobert:\n",
    "    vocabs = wobert.readlines()\n",
    "\n",
    "cedict_entry = []\n",
    "for i in contents:\n",
    "    i = i.strip()\n",
    "    entry = i.split(\" \")[0]\n",
    "    cedict_entry.append(entry)\n",
    "\n",
    "wobert_vocab = []\n",
    "for w in vocabs:\n",
    "    w = w.strip()\n",
    "    wobert_vocab.append(w)\n",
    "\n",
    "retD = list(set(wobert_vocab).difference(set(cedict_entry)))\n",
    "for k,g in enumerate(retD):\n",
    "    if is_contain_chinese(g):\n",
    "        pass\n",
    "    else:\n",
    "        del retD[k]\n",
    "\n",
    "print(len(retD))\n",
    "# print(retD)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7b4b4feff2f24a0f0a34464dbe537a36fda679851528fb8735cb41fa49dffb2d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
