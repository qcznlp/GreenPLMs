{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    ".text_cell_render {\n",
    "font-family: Times New Roman, serif;\n",
    "}\n",
    "</style>\n",
    "**Part 1: Translate English BERT-Large-Whole-Word-Masking Vocabulary into Chinese via ECDICT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Generate English-Chinese dictionary via ecdict.\n",
    "# ecdict is from https://github.com/skywind3000/ECDICT\n",
    "df = pd.read_csv(\"E:\\\\Steve_Zeng_Related\\\\YLab\\\\Translation_BERT_project\\\\code_project\\\\data\\\\ecdict.txt\",sep=\",\")\n",
    "zh_en_dict = dict(zip(df.word, df.translation))\n",
    "\n",
    "def read_vocabulary_and_translate(vocab_path,new_path,bi_dict):\n",
    "    '''\n",
    "    Accepts a BERT-wwm vocabulary and generates its translation\n",
    "    vocab_path: BERT-wwm vocabulary path\n",
    "    new_path: Translated vocabulary path\n",
    "    bi_dict: Bilingual dictionary, such as zh_en_dict above\n",
    "    '''\n",
    "    word_list = []\n",
    "    new_word_list = []\n",
    "    with open(vocab_path,encoding=\"UTF-8\") as vocab:\n",
    "        vocab_contents = vocab.readlines()\n",
    "        for line in vocab_contents:\n",
    "            word = line.strip()\n",
    "            word_list.append(word)\n",
    "            if word.isdigit():\n",
    "                new_word = word\n",
    "            elif word in zh_en_dict.keys():\n",
    "                new_word = bi_dict[word]\n",
    "            elif word.lower() in zh_en_dict.keys():\n",
    "                new_word = bi_dict[word.lower()]\n",
    "            else:\n",
    "                new_word = word\n",
    "            new_word_list.append(new_word)\n",
    "    with open(new_path,'a',encoding=\"UTF-8\") as obj:\n",
    "        for item in new_word_list:\n",
    "            obj.write(item + \"\\n\")\n",
    "\n",
    "# Generate trans.txt\n",
    "read_vocabulary_and_translate(\"E:\\\\Steve_Zeng_Related\\\\YLab\\\\Translation_BERT_project\\\\code_project\\\\BERT\\\\bert-large-cased-whole-word-masking\\\\vocab.txt\",\n",
    "                            \"E:\\\\Steve_Zeng_Related\\\\YLab\\\\Translation_BERT_project\\\\code_project\\\\data\\\\trans_dict.txt\",zh_en_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    ".text_cell_render {\n",
    "font-family: Times New Roman, serif;\n",
    "}\n",
    "</style>\n",
    "**Part 2: Process Chinese-English Dictionary into available Python data structure and clean the dictionary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Chinese-English dictionary\n",
    "# Dictionary is from https://www.mdbg.net/chinese/dictionary?page=cedict\n",
    "import jionlp as jio\n",
    "import re\n",
    "\n",
    "with open('E:\\\\Steve_Zeng_Related\\\\YLab\\\\Translation_BERT_project\\\\code_project\\\\data\\\\cedict_ts.u8',encoding=\"UTF-8\") as file:\n",
    "    cedict = file.readlines()\n",
    "\n",
    "def parse_line(line):\n",
    "    parsed = {}\n",
    "    if line == '':\n",
    "        cedict.remove(line)\n",
    "        return 0\n",
    "    if line.startswith(\"#\"):\n",
    "        cedict.remove(line)\n",
    "        return 0\n",
    "    line = line.rstrip('/')\n",
    "    line = line.split('/')\n",
    "    if len(line) <= 1:\n",
    "        return 0\n",
    "    english = line[1]\n",
    "    if \"/\" in english:\n",
    "        english = english.split(\"/\")[0]\n",
    "    char_and_pinyin = line[0].split('[')\n",
    "    characters = char_and_pinyin[0]\n",
    "    characters = characters.split()\n",
    "    traditional = characters[0]\n",
    "    simplified = characters[1]\n",
    "    pinyin = char_and_pinyin[1]\n",
    "    pinyin = pinyin.rstrip()\n",
    "    pinyin = pinyin.rstrip(\"]\")\n",
    "    parsed['traditional'] = traditional\n",
    "    parsed['simplified'] = simplified\n",
    "    parsed['pinyin'] = pinyin\n",
    "    parsed['english'] = english\n",
    "    return parsed\n",
    "\n",
    "def is_contain_chinese(check_str):\n",
    "    \"\"\"\n",
    "    判断字符串中是否包含中文\n",
    "    :param check_str: {str} 需要检测的字符串\n",
    "    :return: {bool} 包含返回True， 不包含返回False\n",
    "    \"\"\"\n",
    "    for ch in check_str:\n",
    "        if u'\\u4e00' <= ch <= u'\\u9fff':\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def clean_parsed_dict(parsed_dict):\n",
    "    simple_punctuation = '[;|.]'\n",
    "    for k,v in list(parsed_dict.items()):\n",
    "        clean = jio.remove_parentheses(v)\n",
    "        no_punctuation = re.sub(simple_punctuation, '', clean)\n",
    "        parsed_dict[k] = no_punctuation\n",
    "    for key,value in list(parsed_dict.items()):\n",
    "        if is_contain_chinese(value):\n",
    "            del parsed_dict[key]\n",
    "        elif \"lit\" in value:\n",
    "            del parsed_dict[key]\n",
    "        elif \",\" in value:\n",
    "            parsed_dict[key] = value.split(\",\")[0].strip()\n",
    "    return parsed_dict\n",
    "\n",
    "# Generate cedict.txt\n",
    "parsed_dict = {}\n",
    "for line in cedict:\n",
    "    part_parsed = parse_line(line)\n",
    "    simplified = part_parsed['simplified']\n",
    "    english = part_parsed[\"english\"]\n",
    "    parsed_dict[simplified] = english\n",
    "parsed_dict = clean_parsed_dict(parsed_dict)\n",
    "file = open('E:\\\\Steve_Zeng_Related\\\\YLab\\\\Translation_BERT_project\\\\code_project\\\\data\\\\cedict_jio.txt', 'w',encoding=\"UTF-8\") \n",
    "for k,v in parsed_dict.items():\n",
    "    if v != \"\":\n",
    "        file.write(str(k)+' '+str(v)+'\\n')\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    ".text_cell_render {\n",
    "font-family: Times New Roman, serif;\n",
    "}\n",
    "</style>\n",
    "**Part 3: Translate English BERT-base-uncased vocabulary into Chinese via Baidu Translate and clean it**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# This code shows an example of text translation from English to Simplified-Chinese.\n",
    "# This code runs on Python 2.7.x and Python 3.x.\n",
    "# You may install `requests` to run this code: pip install requests\n",
    "# Please refer to `https://api.fanyi.baidu.com/doc/21` for complete api document\n",
    "\n",
    "import requests\n",
    "import random\n",
    "import json\n",
    "from hashlib import md5\n",
    "\n",
    "# Set your own appid/appkey.\n",
    "appid = '20210926000957196'\n",
    "appkey = 'tPgtGyQQuvfgsAMbbUvK'\n",
    "\n",
    "# For list of language codes, please refer to `https://api.fanyi.baidu.com/doc/21`\n",
    "from_lang = 'en'\n",
    "to_lang =  'zh'\n",
    "\n",
    "endpoint = 'http://api.fanyi.baidu.com'\n",
    "path = '/api/trans/vip/translate'\n",
    "url = endpoint + path\n",
    "\n",
    "# query = 'Hello World! This is 1st paragraph.\\nThis is 2nd paragraph.'\n",
    "\n",
    "# Generate salt and sign\n",
    "def make_md5(s, encoding='utf-8'):\n",
    "    return md5(s.encode(encoding)).hexdigest()\n",
    "\n",
    "new_word_list = []\n",
    "with open(\"E:\\\\Steve_Zeng_Related\\\\YLab\\\\Translation_BERT_project\\\\code_project\\\\BERT\\\\bert-base-uncased\\\\vocab.txt\",\"r\",encoding=\"UTF-8\") as file:\n",
    "    contents = file.readlines()\n",
    "for item in contents:\n",
    "    item = item.strip()\n",
    "    if item.startswith(\"##\"):\n",
    "        new_word_list.append(item)\n",
    "        continue\n",
    "    elif item.startswith(\"[\"):\n",
    "        new_word_list.append(item)\n",
    "        continue\n",
    "    else:\n",
    "        salt = random.randint(32768, 65536)\n",
    "        sign = make_md5(appid + item + str(salt) + appkey)\n",
    "        headers = {'Content-Type': 'application/x-www-form-urlencoded'}\n",
    "        payload = {'appid': appid, 'q': item, 'from': from_lang, 'to': to_lang, 'salt': salt, 'sign': sign}\n",
    "        r = requests.post(url, params=payload, headers=headers)\n",
    "        result = r.json()\n",
    "        json_string = json.dumps(result, indent=4, ensure_ascii=False)\n",
    "        string = json.loads(json_string)\n",
    "        try:\n",
    "            trans_word = string[\"trans_result\"][0][\"dst\"]\n",
    "            new_word_list.append(trans_word)\n",
    "        except KeyError:\n",
    "            new_word_list.append(item)\n",
    "    \n",
    "    with open(\"E:\\\\Steve_Zeng_Related\\\\YLab\\\\Translation_BERT_project\\\\code_project\\\\data\\\\BERT-base-uncased-trans.txt\",\"w\",encoding=\"UTF-8\") as file:\n",
    "        for new_word in new_word_list:\n",
    "            file.write(new_word + \"\\n\")\n",
    "\n",
    "import texthero as hero\n",
    "import jionlp as jio\n",
    "import re\n",
    "\n",
    "processed = []\n",
    "with open(\"E:\\\\Steve_Zeng_Related\\\\YLab\\\\Translation_BERT_project\\\\code_project\\\\data\\\\BERT-base-uncased-trans.txt\",\"r\",encoding=\"UTF-8\") as f:\n",
    "    contents = f.readlines()\n",
    "for i in contents:\n",
    "    i = i.strip()\n",
    "    if len(i) == 1:\n",
    "        processed.append(i)\n",
    "    elif i.startswith(\"[\"):\n",
    "        processed.append(i)\n",
    "    elif i.startswith(\"#\"):\n",
    "        processed.append(i)\n",
    "    else:\n",
    "        no_parenthesis = jio.remove_parentheses(i)\n",
    "        no_punctuation = re.sub(r'[^\\w\\s]', '', no_parenthesis)\n",
    "        no_space = no_punctuation.replace(\" \",\"\")\n",
    "        processed.append(no_space)\n",
    "\n",
    "with open(\"E:\\\\Steve_Zeng_Related\\\\YLab\\\\Translation_BERT_project\\\\code_project\\\\data\\\\BERT-base-uncased-trans-processed.txt\",\"w\",encoding=\"UTF-8\") as file:\n",
    "        for process in processed:\n",
    "            file.write(process + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    ".text_cell_render {\n",
    "font-family: Times New Roman, serif;\n",
    "}\n",
    "</style>\n",
    "**Part 4: Clean English-Chinese dictionary generated from Part 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"E:\\\\Steve_Zeng_Related\\\\YLab\\\\Translation_BERT_project\\\\code_project\\\\data\\\\trans_dict.txt\",\"r\",encoding=\"UTF-8\") as f:\n",
    "    trans_dict_contents = f.readlines()\n",
    "\n",
    "def containenglish(test_string):\n",
    "    return bool(re.search('[a-zA-Z]', test_string))\n",
    "\n",
    "without_line = []\n",
    "for item in trans_dict_contents:\n",
    "    item = item.strip()\n",
    "    if \"\\\\\" in item:\n",
    "        new_item = item.split(\"\\\\\")[0].strip()\n",
    "        without_line.append(new_item)\n",
    "    else:\n",
    "        without_line.append(item)\n",
    "\n",
    "no_role = []\n",
    "for no_l in without_line:\n",
    "    if len(no_l) != 1 and \".\" in no_l:\n",
    "        without_role = no_l.split(\".\")[1]\n",
    "        no_role.append(without_role)\n",
    "    else:\n",
    "        no_role.append(no_l)\n",
    "    \n",
    "no_brackets = []\n",
    "for no_r in no_role:\n",
    "    if is_contain_chinese(no_r):\n",
    "        without_b = jio.remove_parentheses(no_r)\n",
    "        no_brackets.append(without_b)\n",
    "    else:\n",
    "        no_brackets.append(no_r)\n",
    "\n",
    "no_punctuation = []\n",
    "simple_punctuation = '[;,；，]'\n",
    "for no_b in no_brackets:\n",
    "    without_punctuation = re.sub(simple_punctuation, ' ', no_b)\n",
    "    no_punctuation.append(without_punctuation)\n",
    "\n",
    "no_space = []\n",
    "for no_pu in no_punctuation:\n",
    "    no_pu = no_pu.strip()\n",
    "    if \" \" in no_pu:\n",
    "        without_s = no_pu.split(\" \")[0]\n",
    "        no_space.append(without_s)\n",
    "    else:\n",
    "        no_space.append(no_pu)\n",
    "\n",
    "no_tense_things = []\n",
    "for no_s in no_space:\n",
    "    if containenglish(no_s) and \"的\" in no_s:\n",
    "        without_tense = no_s.split(\"的\")[0]\n",
    "        no_tense_things.append(without_tense)\n",
    "    else:\n",
    "        no_tense_things.append(no_s)\n",
    "\n",
    "with open(\"E:\\\\Steve_Zeng_Related\\\\YLab\\\\Translation_BERT_project\\\\code_project\\\\data\\\\BERT-large-wwm-dict-processed.txt\",\"w\",encoding=\"UTF-8\") as file:\n",
    "        for ntt in no_tense_things:\n",
    "            file.write(ntt + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    ".text_cell_render {\n",
    "font-family: Times New Roman, serif;\n",
    "}\n",
    "</style>\n",
    "**Part 5: Check the difference between CEDICT-jio and WoBERT vocabulary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('E:\\\\Steve_Zeng_Related\\\\YLab\\\\Translation_BERT_project\\\\code_project\\\\data\\\\cedict_jio.txt', 'r',encoding=\"UTF-8\") as cedictjio:\n",
    "    contents = cedictjio.readlines()\n",
    "\n",
    "with open('E:\\\\Steve_Zeng_Related\\\\YLab\\\\Translation_BERT_project\\\\code_project\\\\BERT\\\\chinese_wobert_L-12_H-768_A-12\\\\vocab.txt', 'r',encoding=\"UTF-8\") as wobert:\n",
    "    vocabs = wobert.readlines()\n",
    "\n",
    "cedict_entry = []\n",
    "for i in contents:\n",
    "    i = i.strip()\n",
    "    entry = i.split(\" \")[0]\n",
    "    cedict_entry.append(entry)\n",
    "\n",
    "wobert_vocab = []\n",
    "for w in vocabs:\n",
    "    w = w.strip()\n",
    "    wobert_vocab.append(w)\n",
    "\n",
    "retD = list(set(wobert_vocab).difference(set(cedict_entry)))\n",
    "for k,g in enumerate(retD):\n",
    "    if is_contain_chinese(g):\n",
    "        pass\n",
    "    else:\n",
    "        del retD[k]\n",
    "\n",
    "print(len(retD))\n",
    "# print(retD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    ".text_cell_render {\n",
    "font-family: Times New Roman, serif;\n",
    "}\n",
    "</style>\n",
    "**Part 6: Translate English BERT-base-uncased Vocabulary into Chinese via ECDICT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Generate English-Chinese dictionary via ecdict.\n",
    "# ecdict is from https://github.com/skywind3000/ECDICT\n",
    "df = pd.read_csv(\"E:\\\\Steve_Zeng_Related\\\\YLab\\\\Translation_BERT_project\\\\code_project\\\\data\\\\ecdict.txt\",sep=\",\")\n",
    "zh_en_dict = dict(zip(df.word, df.translation))\n",
    "\n",
    "def read_vocabulary_and_translate(vocab_path,new_path,bi_dict):\n",
    "    '''\n",
    "    Accepts a BERT-wwm vocabulary and generates its translation\n",
    "    vocab_path: BERT-wwm vocabulary path\n",
    "    new_path: Translated vocabulary path\n",
    "    bi_dict: Bilingual dictionary, such as zh_en_dict above\n",
    "    '''\n",
    "    word_list = []\n",
    "    new_word_list = []\n",
    "    with open(vocab_path,encoding=\"UTF-8\") as vocab:\n",
    "        vocab_contents = vocab.readlines()\n",
    "        for line in vocab_contents:\n",
    "            word = line.strip()\n",
    "            word_list.append(word)\n",
    "            if word.isdigit():\n",
    "                new_word = word\n",
    "            elif word in zh_en_dict.keys():\n",
    "                new_word = bi_dict[word]\n",
    "            elif word.lower() in zh_en_dict.keys():\n",
    "                new_word = bi_dict[word.lower()]\n",
    "            else:\n",
    "                new_word = word\n",
    "            new_word_list.append(new_word)\n",
    "    with open(new_path,'a',encoding=\"UTF-8\") as obj:\n",
    "        for item in new_word_list:\n",
    "            obj.write(item + \"\\n\")\n",
    "\n",
    "# Generate trans.txt\n",
    "read_vocabulary_and_translate(\"E:\\\\Steve_Zeng_Related\\\\YLab\\\\Translation_BERT_project\\\\code_project\\\\BERT\\\\bert-base-uncased\\\\vocab.txt\",\n",
    "                            \"E:\\\\Steve_Zeng_Related\\\\YLab\\\\Translation_BERT_project\\\\code_project\\\\data\\\\bert_base_uncased_trans_dict.txt\",zh_en_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    ".text_cell_render {\n",
    "font-family: Times New Roman, serif;\n",
    "}\n",
    "</style>\n",
    "**Part 6: Clean English-Chinese dictionary generated from Part 5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jionlp as jio\n",
    "import re\n",
    "with open(\"E:\\\\Steve_Zeng_Related\\\\YLab\\\\Translation_BERT_project\\\\code_project\\\\data\\\\bert_base_uncased_trans_dict.txt\",\"r\",encoding=\"UTF-8\") as f:\n",
    "    trans_dict_contents = f.readlines()\n",
    "\n",
    "def containenglish(test_string):\n",
    "    return bool(re.search('[a-zA-Z]', test_string))\n",
    "\n",
    "def is_contain_chinese(check_str):\n",
    "    \"\"\"\n",
    "    判断字符串中是否包含中文\n",
    "    :param check_str: {str} 需要检测的字符串\n",
    "    :return: {bool} 包含返回True， 不包含返回False\n",
    "    \"\"\"\n",
    "    for ch in check_str:\n",
    "        if u'\\u4e00' <= ch <= u'\\u9fff':\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "without_line = []\n",
    "for item in trans_dict_contents:\n",
    "    item = item.strip()\n",
    "    if r\"\\n\" in item:\n",
    "        new_item = item.replace(r\"\\n\",\" \")\n",
    "        without_line.append(new_item)\n",
    "    else:\n",
    "        without_line.append(item)\n",
    "\n",
    "no_propes = []\n",
    "for no_l in without_line:\n",
    "    match = re.search(\"[\\\\[\\u4e00-\\u9fa5\\\\]]\",no_l)\n",
    "    if \"[PAD]\" in no_l or \"[MASK]\" in no_l or \"[SEP]\" in no_l or \"[CLS]\" in no_l or \"[UNK]\" in no_l or \"unused\" in no_l:\n",
    "        no_propes.append(no_l)\n",
    "    elif match:\n",
    "        no_proper = no_l.split(\"[\")[0]\n",
    "        no_propes.append(no_proper)\n",
    "    else:\n",
    "        no_propes.append(no_l)\n",
    "\n",
    "no_brackets = []\n",
    "for no_pr in no_propes:\n",
    "    if is_contain_chinese(no_pr):\n",
    "        without_b = jio.remove_parentheses(no_pr)\n",
    "        no_brackets.append(without_b)\n",
    "    else:\n",
    "        no_brackets.append(no_pr)\n",
    "\n",
    "no_role = []\n",
    "for no_br in no_brackets:\n",
    "    no_ro = no_br.replace(\"pron.\",\" \")\n",
    "    no_ro = no_ro.replace(\"n.\",\" \")\n",
    "    no_ro = no_ro.replace(\"a.\",\" \")\n",
    "    no_ro = no_ro.replace(\"adv.\",\" \")\n",
    "    no_ro = no_ro.replace(\"vbl.\",\" \")\n",
    "    no_ro = no_ro.replace(\"v.\",\" \")\n",
    "    no_ro = no_ro.replace(\"vt.\",\" \")\n",
    "    no_ro = no_ro.replace(\"vi.\",\" \")\n",
    "    no_ro = no_ro.replace(\"abbr.\",\" \")\n",
    "    no_ro = no_ro.replace(\"interj.\",\" \")\n",
    "    no_ro = no_ro.replace(\"conj.\",\" \")\n",
    "    no_ro = no_ro.replace(\"art.\",\" \")\n",
    "    no_ro = no_ro.replace(\"prep.\",\" \")\n",
    "    no_ro = no_ro.replace(\"num.\",\" \")\n",
    "    no_ro = no_ro.replace(\"aux.\",\" \")\n",
    "    no_ro = no_ro.replace(\"pl.\",\" \")\n",
    "    no_ro = no_ro.strip()\n",
    "    no_role.append(no_ro)\n",
    "\n",
    "no_punctuation = []\n",
    "simple_punctuation = '[;,；，]'\n",
    "for no_rol in no_role:\n",
    "    without_punctuation = re.sub(simple_punctuation, ' ', no_rol)\n",
    "    no_punctuation.append(without_punctuation)\n",
    "\n",
    "\n",
    "no_space = []\n",
    "for no_pu in no_punctuation:\n",
    "    no_pu = no_pu.strip()\n",
    "    if \" \" in no_pu:\n",
    "        without_s = no_pu.split()\n",
    "        no_space.append(without_s)\n",
    "    else:\n",
    "        no_space.append(no_pu)\n",
    "\n",
    "no_tense_things = []\n",
    "for no_s in no_space:\n",
    "    if type(no_s) == str:\n",
    "        if containenglish(no_s) and \"的\" in no_s:\n",
    "            no_tt = no_s.split(\"的\")[0]\n",
    "            no_tense_things.append(no_tt)\n",
    "        elif is_contain_chinese(no_s) and \"...\" in no_s:\n",
    "            no_s = no_s.replace(\"...\",\"\")\n",
    "            no_tense_things.append(no_s)\n",
    "        else:\n",
    "            no_tense_things.append(no_s)\n",
    "    else:\n",
    "        for i,single in enumerate(no_s):\n",
    "            if containenglish(single) and \"的\" in single:\n",
    "                del no_s[i]\n",
    "            elif \"...\" in single:\n",
    "                no_s[i] = single.replace(\"...\",\"\")\n",
    "            else:\n",
    "                pass\n",
    "        no_tense_things.append(no_s)\n",
    "                \n",
    "# print(no_tense_things)\n",
    "bert_embedding_help_dict = {}\n",
    "for i, item in enumerate(no_tense_things):\n",
    "    bert_embedding_help_dict[i] = item\n",
    "# print(bert_embedding_help_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    ".text_cell_render {\n",
    "font-family: Times New Roman, serif;\n",
    "}\n",
    "</style>\n",
    "**Part 7: Find single word in CEDICT-jio**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32757\n"
     ]
    }
   ],
   "source": [
    "with open(\"E:\\\\Steve_Zeng_Related\\\\YLab\\\\Translation_BERT_project\\\\code_project\\\\data\\\\cedict_jio.txt\",\"r\",encoding=\"UTF-8\") as f:\n",
    "    contents = f.readlines()\n",
    "\n",
    "single_word_corresponding_chinese_list = []\n",
    "for i in contents:\n",
    "    i = i.strip()\n",
    "    chinese = i.split(\" \",1)[0]\n",
    "    english = i.split(\" \",1)[1]\n",
    "\n",
    "    if len(english.split(\" \")) > 1:\n",
    "        pass\n",
    "    else:\n",
    "        single_word_corresponding_chinese_list.append(chinese)\n",
    "\n",
    "print(len(single_word_corresponding_chinese_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21597\n"
     ]
    }
   ],
   "source": [
    "with open(\"E:\\\\Steve_Zeng_Related\\\\YLab\\\\Translation_BERT_project\\\\code_project\\\\data\\\\facebook_dict\\\\zh-en.txt\",\"r\",encoding=\"UTF-8\") as fi:\n",
    "    fb_contents = fi.readlines()\n",
    "\n",
    "fb_word = []\n",
    "for line in fb_contents:\n",
    "    line = line.strip()\n",
    "    chinese = line.split(\" \")[0]\n",
    "    fb_word.append(chinese)\n",
    "\n",
    "print(len(fb_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_list = fb_word + single_word_corresponding_chinese_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43319\n"
     ]
    }
   ],
   "source": [
    "new_list = list(set(new_list))\n",
    "print(len(new_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202591\n"
     ]
    }
   ],
   "source": [
    "with open(\"E:\\\\Steve_Zeng_Related\\\\YLab\\\\Translation_BERT_project\\\\code_project\\\\data\\\\big_vocab.txt\",\"r\",encoding=\"UTF-8\") as jieba:\n",
    "    jieba_contents = jieba.readlines()\n",
    "\n",
    "jieba_list = [i.strip() for i in jieba_contents]\n",
    "new_list = list(set(new_list + jieba_list))\n",
    "print(len(new_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186445\n",
      "234169\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nwith open(\"E:\\\\Steve_Zeng_Related\\\\YLab\\\\Translation_BERT_project\\\\code_project\\\\data\\\\full_vocab.txt\",\"w\",encoding=\"UTF-8\") as full:\\n    for v in full_vocab:\\n        full.write(v + \"\\n\")\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"E:\\\\Steve_Zeng_Related\\\\YLab\\\\Translation_BERT_project\\\\code_project\\\\BERT\\\\bert-base-uncased-embedding-changed\\\\vocab.txt\",\"r\",encoding=\"UTF-8\") as bert_embedding:\n",
    "    bert_contents = bert_embedding.readlines()\n",
    "\n",
    "bert_vocab = [i.strip() for i in bert_contents]\n",
    "not_in_bert = list(set(new_list).difference(set(bert_vocab)))\n",
    "print(len(not_in_bert))\n",
    "\n",
    "full_vocab = bert_vocab + not_in_bert\n",
    "print(len(full_vocab))\n",
    "'''\n",
    "with open(\"E:\\\\Steve_Zeng_Related\\\\YLab\\\\Translation_BERT_project\\\\code_project\\\\data\\\\full_vocab.txt\",\"w\",encoding=\"UTF-8\") as full:\n",
    "    for v in full_vocab:\n",
    "        full.write(v + \"\\n\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(234169, 768)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (3): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (4): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (5): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (6): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (7): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (8): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (9): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (10): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (11): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "from tokenization_wobert import WoBertTokenizer\n",
    "\n",
    "model = BertModel.from_pretrained(\"E:\\\\Steve_Zeng_Related\\\\YLab\\\\Translation_BERT_project\\\\code_project\\\\BERT\\\\bert-base-uncased-embedding-changed\")\n",
    "model.resize_token_embeddings(len(full_vocab))\n",
    "print(model)\n",
    "model.save_pretrained(\"E:\\\\Steve_Zeng_Related\\\\YLab\\\\Translation_BERT_project\\\\code_project\\\\BERT\\\\bert-base-uncased-embedding-changed-full-vocab\")\n",
    "# tokenizer.save_pretrained(\"E:\\\\Steve_Zeng_Related\\\\YLab\\\\Translation_BERT_project\\\\code_project\\\\BERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22577\n",
      "70302\n"
     ]
    }
   ],
   "source": [
    "with open(\"E:\\\\Steve_Zeng_Related\\\\YLab\\\\Translation_BERT_project\\\\code_project\\\\BERT\\\\bert-base-uncased-embedding-changed\\\\vocab.txt\",\"r\",encoding=\"UTF-8\") as bert_47000:\n",
    "    bert_contents = bert_47000.readlines()\n",
    "\n",
    "bert_list = [i.strip() for i in bert_contents]\n",
    "\n",
    "with open(\"E:\\\\Steve_Zeng_Related\\\\YLab\\\\Translation_BERT_project\\\\code_project\\\\BERT\\\\chinese_wobert_L-12_H-768_A-12\\\\vocab.txt\",\"r\",encoding=\"UTF-8\") as wobert:\n",
    "    wobert_contents = wobert.readlines()\n",
    "\n",
    "wobert_list = [i.strip() for i in wobert_contents]\n",
    "not_in_bert_47000 = list(set(wobert_list).difference(set(bert_list)))\n",
    "print(len(not_in_bert_47000))\n",
    "\n",
    "vocab = bert_list + not_in_bert_47000\n",
    "print(len(vocab))\n",
    "\n",
    "with open(\"E:\\\\Steve_Zeng_Related\\\\YLab\\\\Translation_BERT_project\\\\code_project\\\\data\\\\full_vocab.txt\",\"w\",encoding=\"UTF-8\") as full:\n",
    "    for v in vocab:\n",
    "        full.write(v + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\QINGCH~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.103 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(70302, 768)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (3): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (4): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (5): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (6): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (7): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (8): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (9): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (10): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (11): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "from tokenization_wobert import WoBertTokenizer\n",
    "\n",
    "model = BertModel.from_pretrained(\"E:\\\\Steve_Zeng_Related\\\\YLab\\\\Translation_BERT_project\\\\code_project\\\\BERT\\\\bert-base-uncased-embedding-changed\")\n",
    "model.resize_token_embeddings(len(vocab))\n",
    "print(model)\n",
    "model.save_pretrained(\"E:\\\\Steve_Zeng_Related\\\\YLab\\\\Translation_BERT_project\\\\code_project\\\\BERT\\\\bert-base-uncased-embedding-changed-full-vocab\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7b4b4feff2f24a0f0a34464dbe537a36fda679851528fb8735cb41fa49dffb2d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
